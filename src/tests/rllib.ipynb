{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "RayContext(dashboard_url='', python_version='3.8.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:58432', 'raylet_socket_name': 'tcp://127.0.0.1:54249', 'webui_url': '', 'session_dir': 'C:\\\\Users\\\\lukas\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2022-07-14_09-30-28_303804_11496', 'metrics_export_port': 60869, 'gcs_address': '127.0.0.1:59899', 'address': '127.0.0.1:59899', 'node_id': 'c356f943ece4b1cae8dac534623d8e353be7f1424a112f49c3051bb2'})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=)\u001B[0m 2022-07-14 09:46:46,492\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\python.exe\" C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=50650 --object-store-name=tcp://127.0.0.1:58432 --raylet-name=tcp://127.0.0.1:54249 --redis-address=None --storage=None --temp-dir=C:\\Users\\lukas\\AppData\\Local\\Temp\\ray --metrics-agent-port=58628 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59899 --redis-password=5241590000000000 --startup-token=17 --runtime-env-hash=-704056592\n",
      "\u001B[2m\u001B[36m(pid=)\u001B[0m 2022-07-14 09:46:46,537\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\python.exe\" C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=50650 --object-store-name=tcp://127.0.0.1:58432 --raylet-name=tcp://127.0.0.1:54249 --redis-address=None --storage=None --temp-dir=C:\\Users\\lukas\\AppData\\Local\\Temp\\ray --metrics-agent-port=58628 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59899 --redis-password=5241590000000000 --startup-token=15 --runtime-env-hash=-704056592\n",
      "\u001B[2m\u001B[36m(pid=)\u001B[0m 2022-07-14 09:46:46,561\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\python.exe\" C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=50650 --object-store-name=tcp://127.0.0.1:58432 --raylet-name=tcp://127.0.0.1:54249 --redis-address=None --storage=None --temp-dir=C:\\Users\\lukas\\AppData\\Local\\Temp\\ray --metrics-agent-port=58628 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59899 --redis-password=5241590000000000 --startup-token=16 --runtime-env-hash=-704056592\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:46:55,827\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:46:55,828\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:46:55,828\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:47:07,623\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:47:08,601\tINFO trainable.py:159 -- Trainable.setup took 12.776 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:47:08,602\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:08 (running for 00:00:22.32)<br>Memory usage on this node: 13.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(PPOTrainer pid=4580)\u001B[0m 2022-07-14 09:47:10,833\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:13 (running for 00:00:27.38)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 21.43548387096774\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 78.0\n",
      "  episode_reward_mean: 21.43548387096774\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 186\n",
      "  episodes_total: 186\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6646698713302612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028778420761227608\n",
      "          model: {}\n",
      "          policy_loss: -0.041160497814416885\n",
      "          total_loss: 8.966727256774902\n",
      "          vf_explained_var: -0.0305299311876297\n",
      "          vf_loss: 9.002132415771484\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.3875\n",
      "    ram_util_percent: 82.525\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06536791647511206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05097814585672862\n",
      "    mean_inference_ms: 0.851111493070146\n",
      "    mean_raw_obs_processing_ms: 0.11841277370805564\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 21.43548387096774\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 78.0\n",
      "    episode_reward_mean: 21.43548387096774\n",
      "    episode_reward_min: 8.0\n",
      "    episodes_this_iter: 186\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 15\n",
      "      - 15\n",
      "      - 24\n",
      "      - 17\n",
      "      - 18\n",
      "      - 19\n",
      "      - 17\n",
      "      - 29\n",
      "      - 39\n",
      "      - 17\n",
      "      - 25\n",
      "      - 18\n",
      "      - 19\n",
      "      - 20\n",
      "      - 25\n",
      "      - 38\n",
      "      - 9\n",
      "      - 15\n",
      "      - 13\n",
      "      - 25\n",
      "      - 9\n",
      "      - 9\n",
      "      - 18\n",
      "      - 8\n",
      "      - 27\n",
      "      - 19\n",
      "      - 19\n",
      "      - 14\n",
      "      - 37\n",
      "      - 15\n",
      "      - 18\n",
      "      - 15\n",
      "      - 17\n",
      "      - 43\n",
      "      - 38\n",
      "      - 15\n",
      "      - 11\n",
      "      - 19\n",
      "      - 11\n",
      "      - 16\n",
      "      - 16\n",
      "      - 13\n",
      "      - 18\n",
      "      - 32\n",
      "      - 16\n",
      "      - 12\n",
      "      - 30\n",
      "      - 28\n",
      "      - 9\n",
      "      - 15\n",
      "      - 39\n",
      "      - 35\n",
      "      - 15\n",
      "      - 8\n",
      "      - 10\n",
      "      - 17\n",
      "      - 16\n",
      "      - 12\n",
      "      - 16\n",
      "      - 21\n",
      "      - 12\n",
      "      - 23\n",
      "      - 23\n",
      "      - 39\n",
      "      - 14\n",
      "      - 32\n",
      "      - 13\n",
      "      - 30\n",
      "      - 51\n",
      "      - 78\n",
      "      - 15\n",
      "      - 15\n",
      "      - 17\n",
      "      - 12\n",
      "      - 24\n",
      "      - 16\n",
      "      - 23\n",
      "      - 12\n",
      "      - 21\n",
      "      - 14\n",
      "      - 11\n",
      "      - 27\n",
      "      - 18\n",
      "      - 58\n",
      "      - 39\n",
      "      - 22\n",
      "      - 25\n",
      "      - 17\n",
      "      - 20\n",
      "      - 60\n",
      "      - 12\n",
      "      - 15\n",
      "      - 19\n",
      "      - 12\n",
      "      - 13\n",
      "      - 22\n",
      "      - 13\n",
      "      - 20\n",
      "      - 35\n",
      "      - 70\n",
      "      - 9\n",
      "      - 35\n",
      "      - 17\n",
      "      - 16\n",
      "      - 27\n",
      "      - 12\n",
      "      - 48\n",
      "      - 18\n",
      "      - 14\n",
      "      - 18\n",
      "      - 35\n",
      "      - 38\n",
      "      - 11\n",
      "      - 11\n",
      "      - 12\n",
      "      - 11\n",
      "      - 13\n",
      "      - 25\n",
      "      - 14\n",
      "      - 53\n",
      "      - 11\n",
      "      - 12\n",
      "      - 71\n",
      "      - 16\n",
      "      - 15\n",
      "      - 12\n",
      "      - 15\n",
      "      - 11\n",
      "      - 21\n",
      "      - 21\n",
      "      - 18\n",
      "      - 25\n",
      "      - 33\n",
      "      - 20\n",
      "      - 24\n",
      "      - 12\n",
      "      - 15\n",
      "      - 20\n",
      "      - 12\n",
      "      - 19\n",
      "      - 9\n",
      "      - 36\n",
      "      - 12\n",
      "      - 18\n",
      "      - 22\n",
      "      - 17\n",
      "      - 11\n",
      "      - 14\n",
      "      - 19\n",
      "      - 35\n",
      "      - 19\n",
      "      - 38\n",
      "      - 23\n",
      "      - 12\n",
      "      - 16\n",
      "      - 17\n",
      "      - 12\n",
      "      - 31\n",
      "      - 21\n",
      "      - 26\n",
      "      - 21\n",
      "      - 15\n",
      "      - 19\n",
      "      - 21\n",
      "      - 18\n",
      "      - 15\n",
      "      - 23\n",
      "      - 20\n",
      "      - 20\n",
      "      - 9\n",
      "      - 11\n",
      "      - 27\n",
      "      - 16\n",
      "      - 11\n",
      "      - 16\n",
      "      - 53\n",
      "      - 29\n",
      "      - 14\n",
      "      - 20\n",
      "      - 15\n",
      "      - 33\n",
      "      - 38\n",
      "      - 14\n",
      "      - 10\n",
      "      - 18\n",
      "      - 48\n",
      "      episode_reward:\n",
      "      - 15.0\n",
      "      - 15.0\n",
      "      - 24.0\n",
      "      - 17.0\n",
      "      - 18.0\n",
      "      - 19.0\n",
      "      - 17.0\n",
      "      - 29.0\n",
      "      - 39.0\n",
      "      - 17.0\n",
      "      - 25.0\n",
      "      - 18.0\n",
      "      - 19.0\n",
      "      - 20.0\n",
      "      - 25.0\n",
      "      - 38.0\n",
      "      - 9.0\n",
      "      - 15.0\n",
      "      - 13.0\n",
      "      - 25.0\n",
      "      - 9.0\n",
      "      - 9.0\n",
      "      - 18.0\n",
      "      - 8.0\n",
      "      - 27.0\n",
      "      - 19.0\n",
      "      - 19.0\n",
      "      - 14.0\n",
      "      - 37.0\n",
      "      - 15.0\n",
      "      - 18.0\n",
      "      - 15.0\n",
      "      - 17.0\n",
      "      - 43.0\n",
      "      - 38.0\n",
      "      - 15.0\n",
      "      - 11.0\n",
      "      - 19.0\n",
      "      - 11.0\n",
      "      - 16.0\n",
      "      - 16.0\n",
      "      - 13.0\n",
      "      - 18.0\n",
      "      - 32.0\n",
      "      - 16.0\n",
      "      - 12.0\n",
      "      - 30.0\n",
      "      - 28.0\n",
      "      - 9.0\n",
      "      - 15.0\n",
      "      - 39.0\n",
      "      - 35.0\n",
      "      - 15.0\n",
      "      - 8.0\n",
      "      - 10.0\n",
      "      - 17.0\n",
      "      - 16.0\n",
      "      - 12.0\n",
      "      - 16.0\n",
      "      - 21.0\n",
      "      - 12.0\n",
      "      - 23.0\n",
      "      - 23.0\n",
      "      - 39.0\n",
      "      - 14.0\n",
      "      - 32.0\n",
      "      - 13.0\n",
      "      - 30.0\n",
      "      - 51.0\n",
      "      - 78.0\n",
      "      - 15.0\n",
      "      - 15.0\n",
      "      - 17.0\n",
      "      - 12.0\n",
      "      - 24.0\n",
      "      - 16.0\n",
      "      - 23.0\n",
      "      - 12.0\n",
      "      - 21.0\n",
      "      - 14.0\n",
      "      - 11.0\n",
      "      - 27.0\n",
      "      - 18.0\n",
      "      - 58.0\n",
      "      - 39.0\n",
      "      - 22.0\n",
      "      - 25.0\n",
      "      - 17.0\n",
      "      - 20.0\n",
      "      - 60.0\n",
      "      - 12.0\n",
      "      - 15.0\n",
      "      - 19.0\n",
      "      - 12.0\n",
      "      - 13.0\n",
      "      - 22.0\n",
      "      - 13.0\n",
      "      - 20.0\n",
      "      - 35.0\n",
      "      - 70.0\n",
      "      - 9.0\n",
      "      - 35.0\n",
      "      - 17.0\n",
      "      - 16.0\n",
      "      - 27.0\n",
      "      - 12.0\n",
      "      - 48.0\n",
      "      - 18.0\n",
      "      - 14.0\n",
      "      - 18.0\n",
      "      - 35.0\n",
      "      - 38.0\n",
      "      - 11.0\n",
      "      - 11.0\n",
      "      - 12.0\n",
      "      - 11.0\n",
      "      - 13.0\n",
      "      - 25.0\n",
      "      - 14.0\n",
      "      - 53.0\n",
      "      - 11.0\n",
      "      - 12.0\n",
      "      - 71.0\n",
      "      - 16.0\n",
      "      - 15.0\n",
      "      - 12.0\n",
      "      - 15.0\n",
      "      - 11.0\n",
      "      - 21.0\n",
      "      - 21.0\n",
      "      - 18.0\n",
      "      - 25.0\n",
      "      - 33.0\n",
      "      - 20.0\n",
      "      - 24.0\n",
      "      - 12.0\n",
      "      - 15.0\n",
      "      - 20.0\n",
      "      - 12.0\n",
      "      - 19.0\n",
      "      - 9.0\n",
      "      - 36.0\n",
      "      - 12.0\n",
      "      - 18.0\n",
      "      - 22.0\n",
      "      - 17.0\n",
      "      - 11.0\n",
      "      - 14.0\n",
      "      - 19.0\n",
      "      - 35.0\n",
      "      - 19.0\n",
      "      - 38.0\n",
      "      - 23.0\n",
      "      - 12.0\n",
      "      - 16.0\n",
      "      - 17.0\n",
      "      - 12.0\n",
      "      - 31.0\n",
      "      - 21.0\n",
      "      - 26.0\n",
      "      - 21.0\n",
      "      - 15.0\n",
      "      - 19.0\n",
      "      - 21.0\n",
      "      - 18.0\n",
      "      - 15.0\n",
      "      - 23.0\n",
      "      - 20.0\n",
      "      - 20.0\n",
      "      - 9.0\n",
      "      - 11.0\n",
      "      - 27.0\n",
      "      - 16.0\n",
      "      - 11.0\n",
      "      - 16.0\n",
      "      - 53.0\n",
      "      - 29.0\n",
      "      - 14.0\n",
      "      - 20.0\n",
      "      - 15.0\n",
      "      - 33.0\n",
      "      - 38.0\n",
      "      - 14.0\n",
      "      - 10.0\n",
      "      - 18.0\n",
      "      - 48.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06536791647511206\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05097814585672862\n",
      "      mean_inference_ms: 0.851111493070146\n",
      "      mean_raw_obs_processing_ms: 0.11841277370805564\n",
      "  time_since_restore: 5.260177373886108\n",
      "  time_this_iter_s: 5.260177373886108\n",
      "  time_total_s: 5.260177373886108\n",
      "  timers:\n",
      "    learn_throughput: 1324.283\n",
      "    learn_time_ms: 3020.501\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    training_iteration_time_ms: 5254.179\n",
      "    update_time_ms: 2.996\n",
      "  timestamp: 1657784833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:18 (running for 00:00:32.70)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.26018</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.4355</td><td style=\"text-align: right;\">                  78</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           21.4355</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 42.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 167.0\n",
      "  episode_reward_mean: 42.96\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 88\n",
      "  episodes_total: 274\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 98.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 189.0\n",
      "    episode_reward_mean: 98.85\n",
      "    episode_reward_min: 14.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 64\n",
      "      - 14\n",
      "      - 161\n",
      "      - 128\n",
      "      - 123\n",
      "      - 130\n",
      "      - 159\n",
      "      - 24\n",
      "      - 70\n",
      "      - 43\n",
      "      - 91\n",
      "      - 51\n",
      "      - 94\n",
      "      - 116\n",
      "      - 106\n",
      "      - 122\n",
      "      - 189\n",
      "      - 97\n",
      "      - 43\n",
      "      - 152\n",
      "      episode_reward:\n",
      "      - 64.0\n",
      "      - 14.0\n",
      "      - 161.0\n",
      "      - 128.0\n",
      "      - 123.0\n",
      "      - 130.0\n",
      "      - 159.0\n",
      "      - 24.0\n",
      "      - 70.0\n",
      "      - 43.0\n",
      "      - 91.0\n",
      "      - 51.0\n",
      "      - 94.0\n",
      "      - 116.0\n",
      "      - 106.0\n",
      "      - 122.0\n",
      "      - 189.0\n",
      "      - 97.0\n",
      "      - 43.0\n",
      "      - 152.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07161790367566176\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.045562658319579355\n",
      "      mean_inference_ms: 0.9081156357474951\n",
      "      mean_raw_obs_processing_ms: 0.09482574655746909\n",
      "    timesteps_this_iter: 1977\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6121048331260681\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01743723638355732\n",
      "          model: {}\n",
      "          policy_loss: -0.03193356469273567\n",
      "          total_loss: 9.109278678894043\n",
      "          vf_explained_var: -0.02004896104335785\n",
      "          vf_loss: 9.135980606079102\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.13\n",
      "    ram_util_percent: 82.58000000000001\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06893076205067265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054718968829899274\n",
      "    mean_inference_ms: 0.843070834067909\n",
      "    mean_raw_obs_processing_ms: 0.1207441658513373\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 42.96\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 167.0\n",
      "    episode_reward_mean: 42.96\n",
      "    episode_reward_min: 9.0\n",
      "    episodes_this_iter: 88\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 16\n",
      "      - 53\n",
      "      - 29\n",
      "      - 14\n",
      "      - 20\n",
      "      - 15\n",
      "      - 33\n",
      "      - 38\n",
      "      - 14\n",
      "      - 10\n",
      "      - 18\n",
      "      - 48\n",
      "      - 22\n",
      "      - 43\n",
      "      - 112\n",
      "      - 64\n",
      "      - 39\n",
      "      - 49\n",
      "      - 19\n",
      "      - 21\n",
      "      - 46\n",
      "      - 55\n",
      "      - 24\n",
      "      - 41\n",
      "      - 49\n",
      "      - 43\n",
      "      - 11\n",
      "      - 128\n",
      "      - 26\n",
      "      - 49\n",
      "      - 32\n",
      "      - 15\n",
      "      - 36\n",
      "      - 27\n",
      "      - 21\n",
      "      - 43\n",
      "      - 141\n",
      "      - 133\n",
      "      - 42\n",
      "      - 44\n",
      "      - 60\n",
      "      - 59\n",
      "      - 9\n",
      "      - 35\n",
      "      - 23\n",
      "      - 20\n",
      "      - 49\n",
      "      - 14\n",
      "      - 10\n",
      "      - 41\n",
      "      - 41\n",
      "      - 60\n",
      "      - 32\n",
      "      - 35\n",
      "      - 50\n",
      "      - 65\n",
      "      - 120\n",
      "      - 37\n",
      "      - 32\n",
      "      - 37\n",
      "      - 18\n",
      "      - 14\n",
      "      - 25\n",
      "      - 59\n",
      "      - 36\n",
      "      - 79\n",
      "      - 35\n",
      "      - 113\n",
      "      - 35\n",
      "      - 37\n",
      "      - 65\n",
      "      - 54\n",
      "      - 19\n",
      "      - 48\n",
      "      - 23\n",
      "      - 53\n",
      "      - 167\n",
      "      - 40\n",
      "      - 22\n",
      "      - 13\n",
      "      - 32\n",
      "      - 69\n",
      "      - 32\n",
      "      - 31\n",
      "      - 104\n",
      "      - 45\n",
      "      - 14\n",
      "      - 64\n",
      "      - 13\n",
      "      - 50\n",
      "      - 65\n",
      "      - 48\n",
      "      - 19\n",
      "      - 24\n",
      "      - 17\n",
      "      - 37\n",
      "      - 16\n",
      "      - 12\n",
      "      - 67\n",
      "      - 70\n",
      "      episode_reward:\n",
      "      - 16.0\n",
      "      - 53.0\n",
      "      - 29.0\n",
      "      - 14.0\n",
      "      - 20.0\n",
      "      - 15.0\n",
      "      - 33.0\n",
      "      - 38.0\n",
      "      - 14.0\n",
      "      - 10.0\n",
      "      - 18.0\n",
      "      - 48.0\n",
      "      - 22.0\n",
      "      - 43.0\n",
      "      - 112.0\n",
      "      - 64.0\n",
      "      - 39.0\n",
      "      - 49.0\n",
      "      - 19.0\n",
      "      - 21.0\n",
      "      - 46.0\n",
      "      - 55.0\n",
      "      - 24.0\n",
      "      - 41.0\n",
      "      - 49.0\n",
      "      - 43.0\n",
      "      - 11.0\n",
      "      - 128.0\n",
      "      - 26.0\n",
      "      - 49.0\n",
      "      - 32.0\n",
      "      - 15.0\n",
      "      - 36.0\n",
      "      - 27.0\n",
      "      - 21.0\n",
      "      - 43.0\n",
      "      - 141.0\n",
      "      - 133.0\n",
      "      - 42.0\n",
      "      - 44.0\n",
      "      - 60.0\n",
      "      - 59.0\n",
      "      - 9.0\n",
      "      - 35.0\n",
      "      - 23.0\n",
      "      - 20.0\n",
      "      - 49.0\n",
      "      - 14.0\n",
      "      - 10.0\n",
      "      - 41.0\n",
      "      - 41.0\n",
      "      - 60.0\n",
      "      - 32.0\n",
      "      - 35.0\n",
      "      - 50.0\n",
      "      - 65.0\n",
      "      - 120.0\n",
      "      - 37.0\n",
      "      - 32.0\n",
      "      - 37.0\n",
      "      - 18.0\n",
      "      - 14.0\n",
      "      - 25.0\n",
      "      - 59.0\n",
      "      - 36.0\n",
      "      - 79.0\n",
      "      - 35.0\n",
      "      - 113.0\n",
      "      - 35.0\n",
      "      - 37.0\n",
      "      - 65.0\n",
      "      - 54.0\n",
      "      - 19.0\n",
      "      - 48.0\n",
      "      - 23.0\n",
      "      - 53.0\n",
      "      - 167.0\n",
      "      - 40.0\n",
      "      - 22.0\n",
      "      - 13.0\n",
      "      - 32.0\n",
      "      - 69.0\n",
      "      - 32.0\n",
      "      - 31.0\n",
      "      - 104.0\n",
      "      - 45.0\n",
      "      - 14.0\n",
      "      - 64.0\n",
      "      - 13.0\n",
      "      - 50.0\n",
      "      - 65.0\n",
      "      - 48.0\n",
      "      - 19.0\n",
      "      - 24.0\n",
      "      - 17.0\n",
      "      - 37.0\n",
      "      - 16.0\n",
      "      - 12.0\n",
      "      - 67.0\n",
      "      - 70.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06893076205067265\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.054718968829899274\n",
      "      mean_inference_ms: 0.843070834067909\n",
      "      mean_raw_obs_processing_ms: 0.1207441658513373\n",
      "  time_since_restore: 12.550281524658203\n",
      "  time_this_iter_s: 7.290104150772095\n",
      "  time_total_s: 12.550281524658203\n",
      "  timers:\n",
      "    learn_throughput: 1373.176\n",
      "    learn_time_ms: 2912.955\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    training_iteration_time_ms: 5143.335\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1657784841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:26 (running for 00:00:40.10)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.5503</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   42.96</td><td style=\"text-align: right;\">                 167</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             42.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 68.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 240.0\n",
      "  episode_reward_mean: 68.15\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 310\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5737728476524353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00951562449336052\n",
      "          model: {}\n",
      "          policy_loss: -0.021590879186987877\n",
      "          total_loss: 9.37270450592041\n",
      "          vf_explained_var: 0.06618717312812805\n",
      "          vf_loss: 9.391441345214844\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.5375\n",
      "    ram_util_percent: 82.6375\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06964986851618797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0549615449493756\n",
      "    mean_inference_ms: 0.838248023793807\n",
      "    mean_raw_obs_processing_ms: 0.11837555090755433\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 68.15\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 240.0\n",
      "    episode_reward_mean: 68.15\n",
      "    episode_reward_min: 9.0\n",
      "    episodes_this_iter: 36\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 141\n",
      "      - 133\n",
      "      - 42\n",
      "      - 44\n",
      "      - 60\n",
      "      - 59\n",
      "      - 9\n",
      "      - 35\n",
      "      - 23\n",
      "      - 20\n",
      "      - 49\n",
      "      - 14\n",
      "      - 10\n",
      "      - 41\n",
      "      - 41\n",
      "      - 60\n",
      "      - 32\n",
      "      - 35\n",
      "      - 50\n",
      "      - 65\n",
      "      - 120\n",
      "      - 37\n",
      "      - 32\n",
      "      - 37\n",
      "      - 18\n",
      "      - 14\n",
      "      - 25\n",
      "      - 59\n",
      "      - 36\n",
      "      - 79\n",
      "      - 35\n",
      "      - 113\n",
      "      - 35\n",
      "      - 37\n",
      "      - 65\n",
      "      - 54\n",
      "      - 19\n",
      "      - 48\n",
      "      - 23\n",
      "      - 53\n",
      "      - 167\n",
      "      - 40\n",
      "      - 22\n",
      "      - 13\n",
      "      - 32\n",
      "      - 69\n",
      "      - 32\n",
      "      - 31\n",
      "      - 104\n",
      "      - 45\n",
      "      - 14\n",
      "      - 64\n",
      "      - 13\n",
      "      - 50\n",
      "      - 65\n",
      "      - 48\n",
      "      - 19\n",
      "      - 24\n",
      "      - 17\n",
      "      - 37\n",
      "      - 16\n",
      "      - 12\n",
      "      - 67\n",
      "      - 70\n",
      "      - 32\n",
      "      - 142\n",
      "      - 113\n",
      "      - 60\n",
      "      - 178\n",
      "      - 33\n",
      "      - 185\n",
      "      - 30\n",
      "      - 191\n",
      "      - 117\n",
      "      - 145\n",
      "      - 65\n",
      "      - 75\n",
      "      - 29\n",
      "      - 162\n",
      "      - 170\n",
      "      - 195\n",
      "      - 74\n",
      "      - 44\n",
      "      - 85\n",
      "      - 10\n",
      "      - 51\n",
      "      - 206\n",
      "      - 34\n",
      "      - 97\n",
      "      - 43\n",
      "      - 160\n",
      "      - 142\n",
      "      - 162\n",
      "      - 9\n",
      "      - 174\n",
      "      - 90\n",
      "      - 234\n",
      "      - 240\n",
      "      - 13\n",
      "      - 52\n",
      "      episode_reward:\n",
      "      - 141.0\n",
      "      - 133.0\n",
      "      - 42.0\n",
      "      - 44.0\n",
      "      - 60.0\n",
      "      - 59.0\n",
      "      - 9.0\n",
      "      - 35.0\n",
      "      - 23.0\n",
      "      - 20.0\n",
      "      - 49.0\n",
      "      - 14.0\n",
      "      - 10.0\n",
      "      - 41.0\n",
      "      - 41.0\n",
      "      - 60.0\n",
      "      - 32.0\n",
      "      - 35.0\n",
      "      - 50.0\n",
      "      - 65.0\n",
      "      - 120.0\n",
      "      - 37.0\n",
      "      - 32.0\n",
      "      - 37.0\n",
      "      - 18.0\n",
      "      - 14.0\n",
      "      - 25.0\n",
      "      - 59.0\n",
      "      - 36.0\n",
      "      - 79.0\n",
      "      - 35.0\n",
      "      - 113.0\n",
      "      - 35.0\n",
      "      - 37.0\n",
      "      - 65.0\n",
      "      - 54.0\n",
      "      - 19.0\n",
      "      - 48.0\n",
      "      - 23.0\n",
      "      - 53.0\n",
      "      - 167.0\n",
      "      - 40.0\n",
      "      - 22.0\n",
      "      - 13.0\n",
      "      - 32.0\n",
      "      - 69.0\n",
      "      - 32.0\n",
      "      - 31.0\n",
      "      - 104.0\n",
      "      - 45.0\n",
      "      - 14.0\n",
      "      - 64.0\n",
      "      - 13.0\n",
      "      - 50.0\n",
      "      - 65.0\n",
      "      - 48.0\n",
      "      - 19.0\n",
      "      - 24.0\n",
      "      - 17.0\n",
      "      - 37.0\n",
      "      - 16.0\n",
      "      - 12.0\n",
      "      - 67.0\n",
      "      - 70.0\n",
      "      - 32.0\n",
      "      - 142.0\n",
      "      - 113.0\n",
      "      - 60.0\n",
      "      - 178.0\n",
      "      - 33.0\n",
      "      - 185.0\n",
      "      - 30.0\n",
      "      - 191.0\n",
      "      - 117.0\n",
      "      - 145.0\n",
      "      - 65.0\n",
      "      - 75.0\n",
      "      - 29.0\n",
      "      - 162.0\n",
      "      - 170.0\n",
      "      - 195.0\n",
      "      - 74.0\n",
      "      - 44.0\n",
      "      - 85.0\n",
      "      - 10.0\n",
      "      - 51.0\n",
      "      - 206.0\n",
      "      - 34.0\n",
      "      - 97.0\n",
      "      - 43.0\n",
      "      - 160.0\n",
      "      - 142.0\n",
      "      - 162.0\n",
      "      - 9.0\n",
      "      - 174.0\n",
      "      - 90.0\n",
      "      - 234.0\n",
      "      - 240.0\n",
      "      - 13.0\n",
      "      - 52.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06964986851618797\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.0549615449493756\n",
      "      mean_inference_ms: 0.838248023793807\n",
      "      mean_raw_obs_processing_ms: 0.11837555090755433\n",
      "  time_since_restore: 17.59353470802307\n",
      "  time_this_iter_s: 5.043253183364868\n",
      "  time_total_s: 17.59353470802307\n",
      "  timers:\n",
      "    learn_throughput: 1369.145\n",
      "    learn_time_ms: 2921.531\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    training_iteration_time_ms: 5107.643\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1657784846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:31 (running for 00:00:45.20)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         17.5935</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.15</td><td style=\"text-align: right;\">                 240</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             68.15</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:36 (running for 00:00:50.30)<br>Memory usage on this node: 13.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         17.5935</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.15</td><td style=\"text-align: right;\">                 240</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             68.15</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 97.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 312.0\n",
      "  episode_reward_mean: 97.34\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 333\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 289.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 488.0\n",
      "    episode_reward_mean: 289.1\n",
      "    episode_reward_min: 177.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 358\n",
      "      - 198\n",
      "      - 488\n",
      "      - 177\n",
      "      - 247\n",
      "      - 488\n",
      "      - 243\n",
      "      - 263\n",
      "      - 205\n",
      "      - 268\n",
      "      - 286\n",
      "      - 247\n",
      "      - 212\n",
      "      - 190\n",
      "      - 244\n",
      "      - 348\n",
      "      - 238\n",
      "      - 343\n",
      "      - 269\n",
      "      - 470\n",
      "      episode_reward:\n",
      "      - 358.0\n",
      "      - 198.0\n",
      "      - 488.0\n",
      "      - 177.0\n",
      "      - 247.0\n",
      "      - 488.0\n",
      "      - 243.0\n",
      "      - 263.0\n",
      "      - 205.0\n",
      "      - 268.0\n",
      "      - 286.0\n",
      "      - 247.0\n",
      "      - 212.0\n",
      "      - 190.0\n",
      "      - 244.0\n",
      "      - 348.0\n",
      "      - 238.0\n",
      "      - 343.0\n",
      "      - 269.0\n",
      "      - 470.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.073303666311441\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.045564647802372564\n",
      "      mean_inference_ms: 0.870043991767254\n",
      "      mean_raw_obs_processing_ms: 0.09620192739152417\n",
      "    timesteps_this_iter: 5782\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5492804646492004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006602393928915262\n",
      "          model: {}\n",
      "          policy_loss: -0.018158698454499245\n",
      "          total_loss: 9.432049751281738\n",
      "          vf_explained_var: 0.06759973615407944\n",
      "          vf_loss: 9.44822883605957\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_env_steps_sampled: 16000\n",
      "    num_env_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 16000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.2\n",
      "    ram_util_percent: 82.52666666666666\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07000069235323475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05489770519906782\n",
      "    mean_inference_ms: 0.834445371584346\n",
      "    mean_raw_obs_processing_ms: 0.11642684574362726\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 97.34\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 312.0\n",
      "    episode_reward_mean: 97.34\n",
      "    episode_reward_min: 9.0\n",
      "    episodes_this_iter: 23\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 37\n",
      "      - 18\n",
      "      - 14\n",
      "      - 25\n",
      "      - 59\n",
      "      - 36\n",
      "      - 79\n",
      "      - 35\n",
      "      - 113\n",
      "      - 35\n",
      "      - 37\n",
      "      - 65\n",
      "      - 54\n",
      "      - 19\n",
      "      - 48\n",
      "      - 23\n",
      "      - 53\n",
      "      - 167\n",
      "      - 40\n",
      "      - 22\n",
      "      - 13\n",
      "      - 32\n",
      "      - 69\n",
      "      - 32\n",
      "      - 31\n",
      "      - 104\n",
      "      - 45\n",
      "      - 14\n",
      "      - 64\n",
      "      - 13\n",
      "      - 50\n",
      "      - 65\n",
      "      - 48\n",
      "      - 19\n",
      "      - 24\n",
      "      - 17\n",
      "      - 37\n",
      "      - 16\n",
      "      - 12\n",
      "      - 67\n",
      "      - 70\n",
      "      - 32\n",
      "      - 142\n",
      "      - 113\n",
      "      - 60\n",
      "      - 178\n",
      "      - 33\n",
      "      - 185\n",
      "      - 30\n",
      "      - 191\n",
      "      - 117\n",
      "      - 145\n",
      "      - 65\n",
      "      - 75\n",
      "      - 29\n",
      "      - 162\n",
      "      - 170\n",
      "      - 195\n",
      "      - 74\n",
      "      - 44\n",
      "      - 85\n",
      "      - 10\n",
      "      - 51\n",
      "      - 206\n",
      "      - 34\n",
      "      - 97\n",
      "      - 43\n",
      "      - 160\n",
      "      - 142\n",
      "      - 162\n",
      "      - 9\n",
      "      - 174\n",
      "      - 90\n",
      "      - 234\n",
      "      - 240\n",
      "      - 13\n",
      "      - 52\n",
      "      - 230\n",
      "      - 252\n",
      "      - 181\n",
      "      - 205\n",
      "      - 93\n",
      "      - 259\n",
      "      - 33\n",
      "      - 76\n",
      "      - 132\n",
      "      - 93\n",
      "      - 172\n",
      "      - 100\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      episode_reward:\n",
      "      - 37.0\n",
      "      - 18.0\n",
      "      - 14.0\n",
      "      - 25.0\n",
      "      - 59.0\n",
      "      - 36.0\n",
      "      - 79.0\n",
      "      - 35.0\n",
      "      - 113.0\n",
      "      - 35.0\n",
      "      - 37.0\n",
      "      - 65.0\n",
      "      - 54.0\n",
      "      - 19.0\n",
      "      - 48.0\n",
      "      - 23.0\n",
      "      - 53.0\n",
      "      - 167.0\n",
      "      - 40.0\n",
      "      - 22.0\n",
      "      - 13.0\n",
      "      - 32.0\n",
      "      - 69.0\n",
      "      - 32.0\n",
      "      - 31.0\n",
      "      - 104.0\n",
      "      - 45.0\n",
      "      - 14.0\n",
      "      - 64.0\n",
      "      - 13.0\n",
      "      - 50.0\n",
      "      - 65.0\n",
      "      - 48.0\n",
      "      - 19.0\n",
      "      - 24.0\n",
      "      - 17.0\n",
      "      - 37.0\n",
      "      - 16.0\n",
      "      - 12.0\n",
      "      - 67.0\n",
      "      - 70.0\n",
      "      - 32.0\n",
      "      - 142.0\n",
      "      - 113.0\n",
      "      - 60.0\n",
      "      - 178.0\n",
      "      - 33.0\n",
      "      - 185.0\n",
      "      - 30.0\n",
      "      - 191.0\n",
      "      - 117.0\n",
      "      - 145.0\n",
      "      - 65.0\n",
      "      - 75.0\n",
      "      - 29.0\n",
      "      - 162.0\n",
      "      - 170.0\n",
      "      - 195.0\n",
      "      - 74.0\n",
      "      - 44.0\n",
      "      - 85.0\n",
      "      - 10.0\n",
      "      - 51.0\n",
      "      - 206.0\n",
      "      - 34.0\n",
      "      - 97.0\n",
      "      - 43.0\n",
      "      - 160.0\n",
      "      - 142.0\n",
      "      - 162.0\n",
      "      - 9.0\n",
      "      - 174.0\n",
      "      - 90.0\n",
      "      - 234.0\n",
      "      - 240.0\n",
      "      - 13.0\n",
      "      - 52.0\n",
      "      - 230.0\n",
      "      - 252.0\n",
      "      - 181.0\n",
      "      - 205.0\n",
      "      - 93.0\n",
      "      - 259.0\n",
      "      - 33.0\n",
      "      - 76.0\n",
      "      - 132.0\n",
      "      - 93.0\n",
      "      - 172.0\n",
      "      - 100.0\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07000069235323475\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05489770519906782\n",
      "      mean_inference_ms: 0.834445371584346\n",
      "      mean_raw_obs_processing_ms: 0.11642684574362726\n",
      "  time_since_restore: 28.7133207321167\n",
      "  time_this_iter_s: 11.119786024093628\n",
      "  time_total_s: 28.7133207321167\n",
      "  timers:\n",
      "    learn_throughput: 1387.269\n",
      "    learn_time_ms: 2883.363\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    training_iteration_time_ms: 5050.853\n",
      "    update_time_ms: 2.47\n",
      "  timestamp: 1657784857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:42 (running for 00:00:56.27)<br>Memory usage on this node: 13.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.6006</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   127.8</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             127.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:47 (running for 00:01:01.38)<br>Memory usage on this node: 13.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.6006</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   127.8</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             127.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:52 (running for 00:01:06.42)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.6006</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   127.8</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             127.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-47-52\n",
      "  done: false\n",
      "  episode_len_mean: 163.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 163.38\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 358\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 251.25\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 251.25\n",
      "    episode_reward_min: 44.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 245\n",
      "      - 227\n",
      "      - 252\n",
      "      - 44\n",
      "      - 249\n",
      "      - 276\n",
      "      - 245\n",
      "      - 218\n",
      "      - 213\n",
      "      - 248\n",
      "      - 194\n",
      "      - 323\n",
      "      - 209\n",
      "      - 254\n",
      "      - 309\n",
      "      - 305\n",
      "      - 241\n",
      "      - 312\n",
      "      - 500\n",
      "      - 161\n",
      "      episode_reward:\n",
      "      - 245.0\n",
      "      - 227.0\n",
      "      - 252.0\n",
      "      - 44.0\n",
      "      - 249.0\n",
      "      - 276.0\n",
      "      - 245.0\n",
      "      - 218.0\n",
      "      - 213.0\n",
      "      - 248.0\n",
      "      - 194.0\n",
      "      - 323.0\n",
      "      - 209.0\n",
      "      - 254.0\n",
      "      - 309.0\n",
      "      - 305.0\n",
      "      - 241.0\n",
      "      - 312.0\n",
      "      - 500.0\n",
      "      - 161.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06914549056654758\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04769018678213873\n",
      "      mean_inference_ms: 0.8688908794404568\n",
      "      mean_raw_obs_processing_ms: 0.09701137296566985\n",
      "    timesteps_this_iter: 5025\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5308126211166382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003426367649808526\n",
      "          model: {}\n",
      "          policy_loss: -0.01240138616412878\n",
      "          total_loss: 9.570040702819824\n",
      "          vf_explained_var: 0.06948088109493256\n",
      "          vf_loss: 9.581928253173828\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_env_steps_sampled: 24000\n",
      "    num_env_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 24000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.333333333333325\n",
      "    ram_util_percent: 82.59333333333335\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06954401940244531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054663113772665944\n",
      "    mean_inference_ms: 0.8276441190325013\n",
      "    mean_raw_obs_processing_ms: 0.11332602241361987\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 163.38\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 163.38\n",
      "    episode_reward_min: 9.0\n",
      "    episodes_this_iter: 12\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 104\n",
      "      - 45\n",
      "      - 14\n",
      "      - 64\n",
      "      - 13\n",
      "      - 50\n",
      "      - 65\n",
      "      - 48\n",
      "      - 19\n",
      "      - 24\n",
      "      - 17\n",
      "      - 37\n",
      "      - 16\n",
      "      - 12\n",
      "      - 67\n",
      "      - 70\n",
      "      - 32\n",
      "      - 142\n",
      "      - 113\n",
      "      - 60\n",
      "      - 178\n",
      "      - 33\n",
      "      - 185\n",
      "      - 30\n",
      "      - 191\n",
      "      - 117\n",
      "      - 145\n",
      "      - 65\n",
      "      - 75\n",
      "      - 29\n",
      "      - 162\n",
      "      - 170\n",
      "      - 195\n",
      "      - 74\n",
      "      - 44\n",
      "      - 85\n",
      "      - 10\n",
      "      - 51\n",
      "      - 206\n",
      "      - 34\n",
      "      - 97\n",
      "      - 43\n",
      "      - 160\n",
      "      - 142\n",
      "      - 162\n",
      "      - 9\n",
      "      - 174\n",
      "      - 90\n",
      "      - 234\n",
      "      - 240\n",
      "      - 13\n",
      "      - 52\n",
      "      - 230\n",
      "      - 252\n",
      "      - 181\n",
      "      - 205\n",
      "      - 93\n",
      "      - 259\n",
      "      - 33\n",
      "      - 76\n",
      "      - 132\n",
      "      - 93\n",
      "      - 172\n",
      "      - 100\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      episode_reward:\n",
      "      - 104.0\n",
      "      - 45.0\n",
      "      - 14.0\n",
      "      - 64.0\n",
      "      - 13.0\n",
      "      - 50.0\n",
      "      - 65.0\n",
      "      - 48.0\n",
      "      - 19.0\n",
      "      - 24.0\n",
      "      - 17.0\n",
      "      - 37.0\n",
      "      - 16.0\n",
      "      - 12.0\n",
      "      - 67.0\n",
      "      - 70.0\n",
      "      - 32.0\n",
      "      - 142.0\n",
      "      - 113.0\n",
      "      - 60.0\n",
      "      - 178.0\n",
      "      - 33.0\n",
      "      - 185.0\n",
      "      - 30.0\n",
      "      - 191.0\n",
      "      - 117.0\n",
      "      - 145.0\n",
      "      - 65.0\n",
      "      - 75.0\n",
      "      - 29.0\n",
      "      - 162.0\n",
      "      - 170.0\n",
      "      - 195.0\n",
      "      - 74.0\n",
      "      - 44.0\n",
      "      - 85.0\n",
      "      - 10.0\n",
      "      - 51.0\n",
      "      - 206.0\n",
      "      - 34.0\n",
      "      - 97.0\n",
      "      - 43.0\n",
      "      - 160.0\n",
      "      - 142.0\n",
      "      - 162.0\n",
      "      - 9.0\n",
      "      - 174.0\n",
      "      - 90.0\n",
      "      - 234.0\n",
      "      - 240.0\n",
      "      - 13.0\n",
      "      - 52.0\n",
      "      - 230.0\n",
      "      - 252.0\n",
      "      - 181.0\n",
      "      - 205.0\n",
      "      - 93.0\n",
      "      - 259.0\n",
      "      - 33.0\n",
      "      - 76.0\n",
      "      - 132.0\n",
      "      - 93.0\n",
      "      - 172.0\n",
      "      - 100.0\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06954401940244531\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.054663113772665944\n",
      "      mean_inference_ms: 0.8276441190325013\n",
      "      mean_raw_obs_processing_ms: 0.11332602241361987\n",
      "  time_since_restore: 43.99662137031555\n",
      "  time_this_iter_s: 10.39599323272705\n",
      "  time_total_s: 43.99662137031555\n",
      "  timers:\n",
      "    learn_throughput: 1399.837\n",
      "    learn_time_ms: 2857.475\n",
      "    load_throughput: 27861415.998\n",
      "    load_time_ms: 0.144\n",
      "    training_iteration_time_ms: 5004.892\n",
      "    update_time_ms: 2.647\n",
      "  timestamp: 1657784872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:47:58 (running for 00:01:11.73)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.8997</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            200.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:03 (running for 00:01:16.80)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.8997</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            200.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:08 (running for 00:01:21.86)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         48.8997</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            200.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 231.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 231.36\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 380\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 458.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 458.0\n",
      "    episode_reward_min: 335.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 429\n",
      "      - 437\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 471\n",
      "      - 395\n",
      "      - 335\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 359\n",
      "      - 407\n",
      "      - 441\n",
      "      - 500\n",
      "      - 425\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 429.0\n",
      "      - 437.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 471.0\n",
      "      - 395.0\n",
      "      - 335.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 359.0\n",
      "      - 407.0\n",
      "      - 441.0\n",
      "      - 500.0\n",
      "      - 425.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06778792657024307\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04806618506882886\n",
      "      mean_inference_ms: 0.8739947838130316\n",
      "      mean_raw_obs_processing_ms: 0.09208044937352813\n",
      "    timesteps_this_iter: 9160\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5309582948684692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0068072727881371975\n",
      "          model: {}\n",
      "          policy_loss: -0.015457130037248135\n",
      "          total_loss: 9.624197959899902\n",
      "          vf_explained_var: 0.12680578231811523\n",
      "          vf_loss: 9.639143943786621\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_env_steps_sampled: 32000\n",
      "    num_env_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 32000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.638095238095236\n",
      "    ram_util_percent: 82.92857142857142\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0694296686193938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05464785244380845\n",
      "    mean_inference_ms: 0.8218161542461175\n",
      "    mean_raw_obs_processing_ms: 0.11065197850267311\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 231.36\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 231.36\n",
      "    episode_reward_min: 9.0\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 185\n",
      "      - 30\n",
      "      - 191\n",
      "      - 117\n",
      "      - 145\n",
      "      - 65\n",
      "      - 75\n",
      "      - 29\n",
      "      - 162\n",
      "      - 170\n",
      "      - 195\n",
      "      - 74\n",
      "      - 44\n",
      "      - 85\n",
      "      - 10\n",
      "      - 51\n",
      "      - 206\n",
      "      - 34\n",
      "      - 97\n",
      "      - 43\n",
      "      - 160\n",
      "      - 142\n",
      "      - 162\n",
      "      - 9\n",
      "      - 174\n",
      "      - 90\n",
      "      - 234\n",
      "      - 240\n",
      "      - 13\n",
      "      - 52\n",
      "      - 230\n",
      "      - 252\n",
      "      - 181\n",
      "      - 205\n",
      "      - 93\n",
      "      - 259\n",
      "      - 33\n",
      "      - 76\n",
      "      - 132\n",
      "      - 93\n",
      "      - 172\n",
      "      - 100\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      episode_reward:\n",
      "      - 185.0\n",
      "      - 30.0\n",
      "      - 191.0\n",
      "      - 117.0\n",
      "      - 145.0\n",
      "      - 65.0\n",
      "      - 75.0\n",
      "      - 29.0\n",
      "      - 162.0\n",
      "      - 170.0\n",
      "      - 195.0\n",
      "      - 74.0\n",
      "      - 44.0\n",
      "      - 85.0\n",
      "      - 10.0\n",
      "      - 51.0\n",
      "      - 206.0\n",
      "      - 34.0\n",
      "      - 97.0\n",
      "      - 43.0\n",
      "      - 160.0\n",
      "      - 142.0\n",
      "      - 162.0\n",
      "      - 9.0\n",
      "      - 174.0\n",
      "      - 90.0\n",
      "      - 234.0\n",
      "      - 240.0\n",
      "      - 13.0\n",
      "      - 52.0\n",
      "      - 230.0\n",
      "      - 252.0\n",
      "      - 181.0\n",
      "      - 205.0\n",
      "      - 93.0\n",
      "      - 259.0\n",
      "      - 33.0\n",
      "      - 76.0\n",
      "      - 132.0\n",
      "      - 93.0\n",
      "      - 172.0\n",
      "      - 100.0\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0694296686193938\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05464785244380845\n",
      "      mean_inference_ms: 0.8218161542461175\n",
      "      mean_raw_obs_processing_ms: 0.11065197850267311\n",
      "  time_since_restore: 63.75907301902771\n",
      "  time_this_iter_s: 14.859407186508179\n",
      "  time_total_s: 63.75907301902771\n",
      "  timers:\n",
      "    learn_throughput: 1406.617\n",
      "    learn_time_ms: 2843.703\n",
      "    load_throughput: 37148554.664\n",
      "    load_time_ms: 0.108\n",
      "    training_iteration_time_ms: 4980.078\n",
      "    update_time_ms: 2.609\n",
      "  timestamp: 1657784892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:17 (running for 00:01:31.55)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         68.6591</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  262.58</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            262.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:22 (running for 00:01:36.65)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         68.6591</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  262.58</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            262.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:28 (running for 00:01:41.71)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         68.6591</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  262.58</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            262.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:33 (running for 00:01:46.76)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         68.6591</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  262.58</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            262.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 294.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 294.51\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 398\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 464.75\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 464.75\n",
      "    episode_reward_min: 138.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 302\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 138\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 302.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 138.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07078347682342337\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.050275769948043524\n",
      "      mean_inference_ms: 0.8820216344352263\n",
      "      mean_raw_obs_processing_ms: 0.09297611771411382\n",
      "    timesteps_this_iter: 9295\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5021913647651672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007825558073818684\n",
      "          model: {}\n",
      "          policy_loss: -0.020310088992118835\n",
      "          total_loss: 9.557952880859375\n",
      "          vf_explained_var: 0.02580210752785206\n",
      "          vf_loss: 9.57796859741211\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_env_steps_sampled: 40000\n",
      "    num_env_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 40000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.277272727272727\n",
      "    ram_util_percent: 82.88181818181819\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06982577757599666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05467550988451833\n",
      "    mean_inference_ms: 0.8188211551920389\n",
      "    mean_raw_obs_processing_ms: 0.10879718827967139\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 294.51\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 294.51\n",
      "    episode_reward_min: 9.0\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 97\n",
      "      - 43\n",
      "      - 160\n",
      "      - 142\n",
      "      - 162\n",
      "      - 9\n",
      "      - 174\n",
      "      - 90\n",
      "      - 234\n",
      "      - 240\n",
      "      - 13\n",
      "      - 52\n",
      "      - 230\n",
      "      - 252\n",
      "      - 181\n",
      "      - 205\n",
      "      - 93\n",
      "      - 259\n",
      "      - 33\n",
      "      - 76\n",
      "      - 132\n",
      "      - 93\n",
      "      - 172\n",
      "      - 100\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 429\n",
      "      - 413\n",
      "      - 500\n",
      "      - 424\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 500\n",
      "      - 101\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 97.0\n",
      "      - 43.0\n",
      "      - 160.0\n",
      "      - 142.0\n",
      "      - 162.0\n",
      "      - 9.0\n",
      "      - 174.0\n",
      "      - 90.0\n",
      "      - 234.0\n",
      "      - 240.0\n",
      "      - 13.0\n",
      "      - 52.0\n",
      "      - 230.0\n",
      "      - 252.0\n",
      "      - 181.0\n",
      "      - 205.0\n",
      "      - 93.0\n",
      "      - 259.0\n",
      "      - 33.0\n",
      "      - 76.0\n",
      "      - 132.0\n",
      "      - 93.0\n",
      "      - 172.0\n",
      "      - 100.0\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 429.0\n",
      "      - 413.0\n",
      "      - 500.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 500.0\n",
      "      - 101.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06982577757599666\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05467550988451833\n",
      "      mean_inference_ms: 0.8188211551920389\n",
      "      mean_raw_obs_processing_ms: 0.10879718827967139\n",
      "  time_since_restore: 84.20480179786682\n",
      "  time_this_iter_s: 15.545740365982056\n",
      "  time_total_s: 84.20480179786682\n",
      "  timers:\n",
      "    learn_throughput: 1406.632\n",
      "    learn_time_ms: 2843.671\n",
      "    load_throughput: 13916071.666\n",
      "    load_time_ms: 0.287\n",
      "    training_iteration_time_ms: 4973.014\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1657784913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:38 (running for 00:01:52.23)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         84.2048</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  294.51</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            294.51</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_env_steps_sampled: 44000\n",
      "    num_env_steps_trained: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-48-38\n",
      "  done: false\n",
      "  episode_len_mean: 324.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 324.1\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 406\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4772644639015198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032786838710308075\n",
      "          model: {}\n",
      "          policy_loss: -0.009290504269301891\n",
      "          total_loss: 9.553754806518555\n",
      "          vf_explained_var: 0.0070154378190636635\n",
      "          vf_loss: 9.562923431396484\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_env_steps_sampled: 44000\n",
      "    num_env_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 44000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.3625\n",
      "    ram_util_percent: 82.86250000000001\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0698217272488717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05458687136385313\n",
      "    mean_inference_ms: 0.8183133634812867\n",
      "    mean_raw_obs_processing_ms: 0.10796717052982518\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 324.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 324.1\n",
      "    episode_reward_min: 13.0\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 234\n",
      "      - 240\n",
      "      - 13\n",
      "      - 52\n",
      "      - 230\n",
      "      - 252\n",
      "      - 181\n",
      "      - 205\n",
      "      - 93\n",
      "      - 259\n",
      "      - 33\n",
      "      - 76\n",
      "      - 132\n",
      "      - 93\n",
      "      - 172\n",
      "      - 100\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 429\n",
      "      - 413\n",
      "      - 500\n",
      "      - 424\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 500\n",
      "      - 101\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 419\n",
      "      episode_reward:\n",
      "      - 234.0\n",
      "      - 240.0\n",
      "      - 13.0\n",
      "      - 52.0\n",
      "      - 230.0\n",
      "      - 252.0\n",
      "      - 181.0\n",
      "      - 205.0\n",
      "      - 93.0\n",
      "      - 259.0\n",
      "      - 33.0\n",
      "      - 76.0\n",
      "      - 132.0\n",
      "      - 93.0\n",
      "      - 172.0\n",
      "      - 100.0\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 429.0\n",
      "      - 413.0\n",
      "      - 500.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 500.0\n",
      "      - 101.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 419.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0698217272488717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05458687136385313\n",
      "      mean_inference_ms: 0.8183133634812867\n",
      "      mean_raw_obs_processing_ms: 0.10796717052982518\n",
      "  time_since_restore: 89.50106167793274\n",
      "  time_this_iter_s: 5.296259880065918\n",
      "  time_total_s: 89.50106167793274\n",
      "  timers:\n",
      "    learn_throughput: 1414.707\n",
      "    learn_time_ms: 2827.44\n",
      "    load_throughput: 13916071.666\n",
      "    load_time_ms: 0.287\n",
      "    training_iteration_time_ms: 4976.522\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1657784918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:43 (running for 00:01:57.63)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         89.5011</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   324.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             324.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:48 (running for 00:02:02.67)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         89.5011</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   324.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             324.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:48:54 (running for 00:02:07.73)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         89.5011</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   324.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             324.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 48000\n",
      "    num_env_steps_trained: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-48-54\n",
      "  done: false\n",
      "  episode_len_mean: 348.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 348.93\n",
      "  episode_reward_min: 33.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 414\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07028097556009669\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.049297683569670424\n",
      "      mean_inference_ms: 0.885555388046398\n",
      "      mean_raw_obs_processing_ms: 0.09454782441561946\n",
      "    timesteps_this_iter: 10000\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4962245225906372\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006057862192392349\n",
      "          model: {}\n",
      "          policy_loss: -0.012860284186899662\n",
      "          total_loss: 9.542266845703125\n",
      "          vf_explained_var: 0.021848440170288086\n",
      "          vf_loss: 9.555013656616211\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_env_steps_sampled: 48000\n",
      "    num_env_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 48000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.756521739130434\n",
      "    ram_util_percent: 83.23913043478261\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07008083038888425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.054771257396106915\n",
      "    mean_inference_ms: 0.8177132544928424\n",
      "    mean_raw_obs_processing_ms: 0.10738243860565584\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 348.93\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 348.93\n",
      "    episode_reward_min: 33.0\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 93\n",
      "      - 259\n",
      "      - 33\n",
      "      - 76\n",
      "      - 132\n",
      "      - 93\n",
      "      - 172\n",
      "      - 100\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 429\n",
      "      - 413\n",
      "      - 500\n",
      "      - 424\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 500\n",
      "      - 101\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 419\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 390\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 93.0\n",
      "      - 259.0\n",
      "      - 33.0\n",
      "      - 76.0\n",
      "      - 132.0\n",
      "      - 93.0\n",
      "      - 172.0\n",
      "      - 100.0\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 429.0\n",
      "      - 413.0\n",
      "      - 500.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 500.0\n",
      "      - 101.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 419.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 390.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07008083038888425\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.054771257396106915\n",
      "      mean_inference_ms: 0.8177132544928424\n",
      "      mean_raw_obs_processing_ms: 0.10738243860565584\n",
      "  time_since_restore: 105.64047265052795\n",
      "  time_this_iter_s: 16.139410972595215\n",
      "  time_total_s: 105.64047265052795\n",
      "  timers:\n",
      "    learn_throughput: 1414.618\n",
      "    learn_time_ms: 2827.619\n",
      "    load_throughput: 13916071.666\n",
      "    load_time_ms: 0.287\n",
      "    training_iteration_time_ms: 4971.706\n",
      "    update_time_ms: 2.588\n",
      "  timestamp: 1657784934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:00 (running for 00:02:13.79)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          105.64</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  348.93</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  33</td><td style=\"text-align: right;\">            348.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_env_steps_sampled: 52000\n",
      "    num_env_steps_trained: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-49-00\n",
      "  done: false\n",
      "  episode_len_mean: 379.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 379.35\n",
      "  episode_reward_min: 33.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 422\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5109347105026245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038389067631214857\n",
      "          model: {}\n",
      "          policy_loss: -0.009485170245170593\n",
      "          total_loss: 9.540412902832031\n",
      "          vf_explained_var: 0.02479461021721363\n",
      "          vf_loss: 9.549825668334961\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_env_steps_sampled: 52000\n",
      "    num_env_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 52000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.628571428571426\n",
      "    ram_util_percent: 83.52857142857144\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07060210094328215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05524582153626663\n",
      "    mean_inference_ms: 0.8172274190959697\n",
      "    mean_raw_obs_processing_ms: 0.10715485937998054\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 379.35\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 379.35\n",
      "    episode_reward_min: 33.0\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 234\n",
      "      - 175\n",
      "      - 158\n",
      "      - 143\n",
      "      - 312\n",
      "      - 236\n",
      "      - 285\n",
      "      - 307\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 429\n",
      "      - 413\n",
      "      - 500\n",
      "      - 424\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 500\n",
      "      - 101\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 419\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 390\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 234.0\n",
      "      - 175.0\n",
      "      - 158.0\n",
      "      - 143.0\n",
      "      - 312.0\n",
      "      - 236.0\n",
      "      - 285.0\n",
      "      - 307.0\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 429.0\n",
      "      - 413.0\n",
      "      - 500.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 500.0\n",
      "      - 101.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 419.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 390.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07060210094328215\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05524582153626663\n",
      "      mean_inference_ms: 0.8172274190959697\n",
      "      mean_raw_obs_processing_ms: 0.10715485937998054\n",
      "  time_since_restore: 110.73367428779602\n",
      "  time_this_iter_s: 5.093201637268066\n",
      "  time_total_s: 110.73367428779602\n",
      "  timers:\n",
      "    learn_throughput: 1421.007\n",
      "    learn_time_ms: 2814.906\n",
      "    load_throughput: 10332706.781\n",
      "    load_time_ms: 0.387\n",
      "    training_iteration_time_ms: 4976.9\n",
      "    update_time_ms: 2.7\n",
      "  timestamp: 1657784940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:05 (running for 00:02:19.01)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         110.734</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  379.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  33</td><td style=\"text-align: right;\">            379.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:10 (running for 00:02:24.07)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         110.734</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  379.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  33</td><td style=\"text-align: right;\">            379.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 56000\n",
      "    num_env_steps_trained: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 400.85\n",
      "  episode_reward_min: 33.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 430\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 420.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 420.45\n",
      "    episode_reward_min: 286.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 464\n",
      "      - 474\n",
      "      - 374\n",
      "      - 430\n",
      "      - 500\n",
      "      - 432\n",
      "      - 377\n",
      "      - 500\n",
      "      - 286\n",
      "      - 324\n",
      "      - 308\n",
      "      - 380\n",
      "      - 313\n",
      "      - 455\n",
      "      - 430\n",
      "      - 500\n",
      "      - 398\n",
      "      - 464\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 464.0\n",
      "      - 474.0\n",
      "      - 374.0\n",
      "      - 430.0\n",
      "      - 500.0\n",
      "      - 432.0\n",
      "      - 377.0\n",
      "      - 500.0\n",
      "      - 286.0\n",
      "      - 324.0\n",
      "      - 308.0\n",
      "      - 380.0\n",
      "      - 313.0\n",
      "      - 455.0\n",
      "      - 430.0\n",
      "      - 500.0\n",
      "      - 398.0\n",
      "      - 464.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06900594040894621\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.04962521205964257\n",
      "      mean_inference_ms: 0.8856535279625251\n",
      "      mean_raw_obs_processing_ms: 0.09417958858786336\n",
      "    timesteps_this_iter: 8409\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5134892463684082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005721193738281727\n",
      "          model: {}\n",
      "          policy_loss: -0.007949365302920341\n",
      "          total_loss: 9.519739151000977\n",
      "          vf_explained_var: 0.019268520176410675\n",
      "          vf_loss: 9.527634620666504\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_env_steps_sampled: 56000\n",
      "    num_env_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 56000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.360000000000003\n",
      "    ram_util_percent: 83.39\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07086047888271692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05531257327743175\n",
      "    mean_inference_ms: 0.8173192063109406\n",
      "    mean_raw_obs_processing_ms: 0.1067332977825204\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 400.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 400.85\n",
      "    episode_reward_min: 33.0\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 222\n",
      "      - 140\n",
      "      - 33\n",
      "      - 213\n",
      "      - 239\n",
      "      - 500\n",
      "      - 413\n",
      "      - 226\n",
      "      - 119\n",
      "      - 174\n",
      "      - 254\n",
      "      - 332\n",
      "      - 199\n",
      "      - 373\n",
      "      - 391\n",
      "      - 220\n",
      "      - 467\n",
      "      - 432\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 429\n",
      "      - 413\n",
      "      - 500\n",
      "      - 424\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 500\n",
      "      - 101\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 419\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 390\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 222.0\n",
      "      - 140.0\n",
      "      - 33.0\n",
      "      - 213.0\n",
      "      - 239.0\n",
      "      - 500.0\n",
      "      - 413.0\n",
      "      - 226.0\n",
      "      - 119.0\n",
      "      - 174.0\n",
      "      - 254.0\n",
      "      - 332.0\n",
      "      - 199.0\n",
      "      - 373.0\n",
      "      - 391.0\n",
      "      - 220.0\n",
      "      - 467.0\n",
      "      - 432.0\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 429.0\n",
      "      - 413.0\n",
      "      - 500.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 500.0\n",
      "      - 101.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 419.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 390.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07086047888271692\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05531257327743175\n",
      "      mean_inference_ms: 0.8173192063109406\n",
      "      mean_raw_obs_processing_ms: 0.1067332977825204\n",
      "  time_since_restore: 125.03170990943909\n",
      "  time_this_iter_s: 14.298035621643066\n",
      "  time_total_s: 125.03170990943909\n",
      "  timers:\n",
      "    learn_throughput: 1419.308\n",
      "    learn_time_ms: 2818.274\n",
      "    load_throughput: 10332706.781\n",
      "    load_time_ms: 0.387\n",
      "    training_iteration_time_ms: 4995.664\n",
      "    update_time_ms: 2.701\n",
      "  timestamp: 1657784954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:19 (running for 00:02:33.27)<br>Memory usage on this node: 13.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         129.947</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  420.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            420.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:24 (running for 00:02:38.31)<br>Memory usage on this node: 13.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         129.947</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  420.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            420.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:29 (running for 00:02:43.35)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         129.947</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  420.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            420.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:34 (running for 00:02:48.37)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         129.947</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  420.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            420.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_1c6f8_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_env_steps_sampled: 64000\n",
      "    num_env_steps_trained: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-14_09-49-35\n",
      "  done: false\n",
      "  episode_len_mean: 431.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 431.15\n",
      "  episode_reward_min: 101.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 448\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 484.75\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 484.75\n",
      "    episode_reward_min: 376.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 496\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 376\n",
      "      - 500\n",
      "      - 489\n",
      "      - 474\n",
      "      - 500\n",
      "      - 500\n",
      "      - 435\n",
      "      - 500\n",
      "      - 425\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 496.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 376.0\n",
      "      - 500.0\n",
      "      - 489.0\n",
      "      - 474.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 435.0\n",
      "      - 500.0\n",
      "      - 425.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.06866850005098675\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.05006494468098608\n",
      "      mean_inference_ms: 0.8882716977072393\n",
      "      mean_raw_obs_processing_ms: 0.09474532322666335\n",
      "    timesteps_this_iter: 9695\n",
      "  experiment_id: cfb75a7d32af4a23beb65df851b6f093\n",
      "  hostname: Redanien\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47113892436027527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005081453826278448\n",
      "          model: {}\n",
      "          policy_loss: -0.007184110581874847\n",
      "          total_loss: 9.376626968383789\n",
      "          vf_explained_var: 0.04663539677858353\n",
      "          vf_loss: 9.38376235961914\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_env_steps_sampled: 64000\n",
      "    num_env_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 64000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.07826086956521\n",
      "    ram_util_percent: 83.31739130434784\n",
      "  pid: 4580\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07175492628409813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.055967141555691435\n",
      "    mean_inference_ms: 0.8174348896259613\n",
      "    mean_raw_obs_processing_ms: 0.10648784873179963\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 431.15\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 431.15\n",
      "    episode_reward_min: 101.0\n",
      "    episodes_this_iter: 8\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 437\n",
      "      - 199\n",
      "      - 162\n",
      "      - 441\n",
      "      - 322\n",
      "      - 382\n",
      "      - 135\n",
      "      - 321\n",
      "      - 309\n",
      "      - 500\n",
      "      - 198\n",
      "      - 124\n",
      "      - 311\n",
      "      - 327\n",
      "      - 366\n",
      "      - 386\n",
      "      - 338\n",
      "      - 270\n",
      "      - 447\n",
      "      - 209\n",
      "      - 256\n",
      "      - 483\n",
      "      - 405\n",
      "      - 424\n",
      "      - 500\n",
      "      - 452\n",
      "      - 321\n",
      "      - 366\n",
      "      - 382\n",
      "      - 500\n",
      "      - 456\n",
      "      - 355\n",
      "      - 500\n",
      "      - 500\n",
      "      - 429\n",
      "      - 413\n",
      "      - 500\n",
      "      - 424\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 461\n",
      "      - 500\n",
      "      - 101\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 419\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 390\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 339\n",
      "      - 478\n",
      "      - 500\n",
      "      - 500\n",
      "      - 281\n",
      "      - 377\n",
      "      - 297\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 346\n",
      "      - 359\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 437.0\n",
      "      - 199.0\n",
      "      - 162.0\n",
      "      - 441.0\n",
      "      - 322.0\n",
      "      - 382.0\n",
      "      - 135.0\n",
      "      - 321.0\n",
      "      - 309.0\n",
      "      - 500.0\n",
      "      - 198.0\n",
      "      - 124.0\n",
      "      - 311.0\n",
      "      - 327.0\n",
      "      - 366.0\n",
      "      - 386.0\n",
      "      - 338.0\n",
      "      - 270.0\n",
      "      - 447.0\n",
      "      - 209.0\n",
      "      - 256.0\n",
      "      - 483.0\n",
      "      - 405.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 321.0\n",
      "      - 366.0\n",
      "      - 382.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 355.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 429.0\n",
      "      - 413.0\n",
      "      - 500.0\n",
      "      - 424.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 461.0\n",
      "      - 500.0\n",
      "      - 101.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 419.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 390.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 339.0\n",
      "      - 478.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 281.0\n",
      "      - 377.0\n",
      "      - 297.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 346.0\n",
      "      - 359.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.07175492628409813\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.055967141555691435\n",
      "      mean_inference_ms: 0.8174348896259613\n",
      "      mean_raw_obs_processing_ms: 0.10648784873179963\n",
      "  time_since_restore: 145.9569661617279\n",
      "  time_this_iter_s: 16.009541988372803\n",
      "  time_total_s: 145.9569661617279\n",
      "  timers:\n",
      "    learn_throughput: 1413.09\n",
      "    learn_time_ms: 2830.676\n",
      "    load_throughput: 13289936.629\n",
      "    load_time_ms: 0.301\n",
      "    training_iteration_time_ms: 5015.834\n",
      "    update_time_ms: 2.501\n",
      "  timestamp: 1657784975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 1c6f8_00000\n",
      "  warmup_time: 12.787656545639038\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:40 (running for 00:02:54.32)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         150.894</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  443.16</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            443.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 09:49:44,652\tWARNING tune.py:682 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:45 (running for 00:02:59.37)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         150.894</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  443.16</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            443.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Current time: 2022-07-14 09:49:45 (running for 00:02:59.38)<br>Memory usage on this node: 13.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/12 CPUs, 0/1 GPUs, 0.0/3.42 GiB heap, 0.0/1.71 GiB objects<br>Result logdir: C:\\Users\\lukas\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_CartPole-v1_1c6f8_00000</td><td>RUNNING </td><td>127.0.0.1:4580</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         150.894</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  443.16</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 101</td><td style=\"text-align: right;\">            443.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 09:49:56,180\tERROR tune.py:743 -- Trials did not complete: [PPO_CartPole-v1_1c6f8_00000]\n",
      "2022-07-14 09:49:56,181\tINFO tune.py:747 -- Total run time: 189.89 seconds (179.37 seconds for the tuning loop).\n",
      "2022-07-14 09:49:56,182\tWARNING tune.py:753 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "\u001B[2m\u001B[36m(pid=)\u001B[0m 2022-07-14 09:49:56,153\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\python.exe\" C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=50650 --object-store-name=tcp://127.0.0.1:58432 --raylet-name=tcp://127.0.0.1:54249 --redis-address=None --storage=None --temp-dir=C:\\Users\\lukas\\AppData\\Local\\Temp\\ray --metrics-agent-port=58628 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59899 --redis-password=5241590000000000 --startup-token=19 --runtime-env-hash=-704056592\n",
      "\u001B[2m\u001B[36m(pid=)\u001B[0m 2022-07-14 09:49:56,176\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\python.exe\" C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=50650 --object-store-name=tcp://127.0.0.1:58432 --raylet-name=tcp://127.0.0.1:54249 --redis-address=None --storage=None --temp-dir=C:\\Users\\lukas\\AppData\\Local\\Temp\\ray --metrics-agent-port=58628 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59899 --redis-password=5241590000000000 --startup-token=20 --runtime-env-hash=-704056592\n",
      "\u001B[2m\u001B[36m(pid=)\u001B[0m 2022-07-14 09:49:56,217\tINFO context.py:67 -- Exec'ing worker with command: \"C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\python.exe\" C:\\Users\\lukas\\anaconda3\\envs\\ray-rl\\lib\\site-packages\\ray\\workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=50650 --object-store-name=tcp://127.0.0.1:58432 --raylet-name=tcp://127.0.0.1:54249 --redis-address=None --storage=None --temp-dir=C:\\Users\\lukas\\AppData\\Local\\Temp\\ray --metrics-agent-port=58628 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59899 --redis-password=5241590000000000 --startup-token=18 --runtime-env-hash=-704056592\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x1d286f8beb0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\", config={\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    \"evaluation_interval\": 2,\n",
    "    \"evaluation_duration\": 20,\n",
    "    \"num_gpus\": 0\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import gym"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}